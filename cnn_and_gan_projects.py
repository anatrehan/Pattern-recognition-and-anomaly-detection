# -*- coding: utf-8 -*-
"""CNN and GAN projects.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12nU4t7TzYLNCmWvdBnajRKz4-yhKkQKS

# **SENTIMENT ANALYSIS USING CNN**
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
import tkinter as tk
from tkinter import ttk
from tkinter import scrolledtext

tokenizer = None  # Make tokenizer global
model = None  # Make model global
maxlen = 100  # Define maxlen as a global variable

def create_cnn_model(input_length, vocab_size, embedding_dim):
    model = Sequential()
    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))
    model.add(Conv1D(128, 5, activation='relu'))
    model.add(GlobalMaxPooling1D())
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(3, activation='softmax')) # 3 classes for sentiment (negative, neutral, positive)

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

def train_model():
    global tokenizer  # Access the global tokenizer variable
    global model  # Access the global model variable
    try:
        epochs = int(epochs_entry.get())
    except ValueError:
        output_text.insert(tk.END, "Please enter a valid integer value for the number of epochs.\n")
        return

    # Load the dataset
    dataset = pd.read_csv("tweet.csv")

    # Remove rows with empty values in either tweet or score column
    dataset = dataset.dropna(subset=['tweet', 'score'])

    # Preprocessing
    max_words = 10000
    tokenizer = Tokenizer(num_words=max_words)
    tokenizer.fit_on_texts(dataset['tweet'].astype(str))

    X = tokenizer.texts_to_sequences(dataset['tweet'].astype(str))
    X = pad_sequences(X, maxlen=maxlen)
    y = dataset['score'].map({-1: 0, 0: 1, 1: 2})  # Remapping scores to 0, 1, 2

    # Filter out rows with unexpected label values
    unexpected_labels = y[~y.isin([0, 1, 2])]
    if not unexpected_labels.empty:
        output_text.insert(tk.END, "Unexpected label values found: {}\n".format(unexpected_labels.unique()))
        output_text.insert(tk.END, "Rows with unexpected label values:\n")
        output_text.insert(tk.END, "{}\n".format(dataset[~y.isin([0, 1, 2])]))
        return

    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Create and train the CNN model
    embedding_dim = 50
    model = create_cnn_model(maxlen, max_words, embedding_dim)

    output_text.delete(1.0, tk.END)
    for epoch in range(1, epochs + 1):
        output_text.insert(tk.END, f"Epoch {epoch}/{epochs}\n")
        model.fit(X_train, y_train, epochs=1, batch_size=32, validation_split=0.2, verbose=0)
        _, train_accuracy = model.evaluate(X_train, y_train, verbose=0)
        _, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
        output_text.insert(tk.END, f"Training Accuracy: {train_accuracy:.4f}\n")
        output_text.insert(tk.END, f"Testing Accuracy: {test_accuracy:.4f}\n\n")
        output_text.see(tk.END)
        output_text.update()

    # Save the trained model to a file
    model.save("sentiment.h5")
    model = load_model("sentiment.h5")
def predict_sentiment():

    global tokenizer  # Access the global tokenizer variable
    global model  # Access the global model variable
    user_tweet = user_input.get()
    user_tweet_sequence = tokenizer.texts_to_sequences([user_tweet])
    user_tweet_sequence = pad_sequences(user_tweet_sequence, maxlen=maxlen)
    model = load_model("sentiment.h5")
    sentiment_prob = model.predict(user_tweet_sequence)[0]
    sentiment = sentiment_prob.argmax()
    if sentiment == 0:
        output_label.config(text="Negative")
    elif sentiment == 1:
        output_label.config(text="Neutral")
    else:
        output_label.config(text="Positive")

def submit_tweet():
    global model, tokenizer  # Access the global model and tokenizer variables
    if model is None:
        # If the model hasn't been loaded, load it from the saved file
        model = load_model("sentiment.h5")
    if tokenizer is None:
        # If the tokenizer hasn't been loaded, load it
        tokenizer = Tokenizer()
        tokenizer.fit_on_texts([""])  # Dummy fit to avoid AttributeError
    predict_sentiment()
    clear_input_button.config(state=tk.NORMAL)


def clear_input():
    user_input.delete(0, tk.END)
    output_label.config(text="")
    clear_input_button.config(state=tk.DISABLED)

# GUI
root = tk.Tk()
root.title("Sentiment Analysis")
root.geometry("600x400")

epochs_label = ttk.Label(root, text="Number of Epochs:")
epochs_label.pack()
epochs_entry = ttk.Entry(root)
epochs_entry.pack()

train_button = ttk.Button(root, text="Train Model", command=train_model)
train_button.pack(pady=10)

user_input_label = ttk.Label(root, text="Enter your tweet:")
user_input_label.pack()
user_input = ttk.Entry(root)
user_input.pack()

submit_button = ttk.Button(root, text="Submit", command=submit_tweet)
submit_button.pack(pady=10)

clear_input_button = ttk.Button(root, text="Clear Input", command=clear_input, state=tk.DISABLED)
clear_input_button.pack(pady=5)

output_label = ttk.Label(root, text="")
output_label.pack(pady=10)

output_text = scrolledtext.ScrolledText(root, width=50, height=10)
output_text.pack(pady=10)

root.mainloop()

"""# **API ANSWERING SYSTEM USING GAN**"""

# Imports
import os
from dotenv import load_dotenv
import streamlit as st
from PIL import Image
import google.generativeai as genai

# Load environment variables
load_dotenv()

# Configure Gemini API
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# Function to load Gemini model and get response
def get_gemini_response(input_data, image):
    if image is None:  # If input is text
        model = genai.GenerativeModel('gemini-pro')  # Use text model
        response = model.generate_content(input_data)
    else:  # If input is an image
        model = genai.GenerativeModel('gemini-pro-vision')  # Use image model
        response = model.generate_content([input_data,image])

    # Check if response is a simple text or a complex structure
    if hasattr(response, 'text'):
        return response.text
    elif hasattr(response, 'parts'):
        # Concatenate text parts
        text_parts = [part.text for part in response.parts]
        return '\n'.join(text_parts)
    else:
        return str(response)  # Return string representation if not recognized format

# Initialize Streamlit app
st.set_page_config(page_title="Anamika's GPT through GEMINI")

# Main UI components
st.header("Anamika's GPT through GEMINI")

# Input field for text prompt
image = None
input_type = st.radio("Input Type:", ("Text", "Image"))
if input_type == "Text":
    input_data = st.text_input("Enter your question:")
elif input_type == "Image":
    input_data = st.text_input("Enter your question:")
    uploaded_file = st.file_uploader("Upload an image...", type=["jpg", "jpeg", "png"])
    if uploaded_file is not None:
        image = Image.open(uploaded_file)
    else:
        image = None

# Button to trigger response generation
submit = st.button("Search")

# If search button is clicked
if submit:
    if input_data or image:  # If input data is provided
        response = get_gemini_response(input_data,image)
        st.subheader("Response:")
        st.write(response)
    else:
        st.warning("Please provide input.")